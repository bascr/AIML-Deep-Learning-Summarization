{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94d04696-9a18-4614-99ed-908c1a91a136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.21.3)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.9/dist-packages (2.4.0)\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting rouge_score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.23.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.12.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (2023.1.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets) (3.8.3)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from datasets) (1.5.0)\n",
      "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (10.0.1)\n",
      "Requirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.3.5.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from datasets) (0.70.13)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.9/dist-packages (from rouge_score) (1.4.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (from rouge_score) (3.7)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/lib/python3/dist-packages (from rouge_score) (1.14.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (18.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.8.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (2.1.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk->rouge_score) (1.2.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk->rouge_score) (8.1.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2022.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Building wheels for collected packages: rouge_score\n",
      "  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24936 sha256=19519dc179f7b4f9636073be5ae42e5377ed653aea337b89659e68d1ced98488\n",
      "  Stored in directory: /root/.cache/pip/wheels/b0/3f/ac/cc3bc304f50c77ef38d79d8e4e2684313de39af543cb4eb3da\n",
      "Successfully built rouge_score\n",
      "Installing collected packages: rouge_score, evaluate\n",
      "Successfully installed evaluate-0.4.1 rouge_score-0.1.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets evaluate rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bad9be70-a9c9-4064-9464-0e5b5a4a58fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from tqdm.notebook import tqdm\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from sklearn.utils import shuffle\n",
    "import evaluate\n",
    "from transformers import AutoTokenizer, DataCollatorForSeq2Seq, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc53875e-3842-423f-9609-b0f0f03cc423",
   "metadata": {},
   "source": [
    "# Prepare dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af283bf-7ef6-487e-b0ed-4afd281adedf",
   "metadata": {},
   "source": [
    "### Creating directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0714ea5b-d1a1-4035-bbf3-5f07bf012704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks/pretrained\n",
      "tokenizer  model  dataset\n"
     ]
    }
   ],
   "source": [
    "DIR=\"t5\"\n",
    "\n",
    "!mkdir -p \"{DIR}/dataset\"\n",
    "!mkdir -p \"{DIR}/model\"\n",
    "!mkdir -p \"{DIR}/tokenizer\"\n",
    "\n",
    "!pwd\n",
    "!ls -r \"{DIR}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152fdfd2-c0b6-44fc-9e31-3181adc7910a",
   "metadata": {},
   "source": [
    "### Getting Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "040333a2-797f-4d40-98e2-a631d09276ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5097824bd39c46e691345178e4e4e098",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/1.53k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55f6ad0978ba472aa5b94b755563eb0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/832 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset billsum/default (download: 64.14 MiB, generated: 259.80 MiB, post-processed: Unknown size, total: 323.94 MiB) to /root/.cache/huggingface/datasets/billsum/default/3.0.0/d1e95173aed3acb71327864be74ead49b578522e4c7206048b2f2e5351b57959...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2246549d544141f6b55fafde85b374f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/67.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/18949 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/3269 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating ca_test split:   0%|          | 0/1237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset billsum downloaded and prepared to /root/.cache/huggingface/datasets/billsum/default/3.0.0/d1e95173aed3acb71327864be74ead49b578522e4c7206048b2f2e5351b57959. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56b74b93a78140f780b420bfa28a48a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"billsum\")\n",
    "\n",
    "train_test_valid = dataset[\"train\"].train_test_split(test_size=0.2, seed=20)\n",
    "test_valid = train_test_valid[\"test\"].train_test_split(test_size=0.5, seed=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cceb2f6-8e1d-45ad-8476-5cb5dcf35fc2",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4517aab-8a25-4653-aa1b-e78d264db6a3",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03ae145c-074e-4e49-8ac2-bd2533bc6c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"t5-small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22d04754-4d60-4d7e-8279-6c0b5a0b43a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87b850beecb2431b8146d508e1d880fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/2.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efcb94b45538475a9b103b80fccd227a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading spiece.model:   0%|          | 0.00/773k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "391ad0ad1f974b128451201d1e6ff617",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/1.32M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b75be47b-7fbc-441a-b03c-a646b1b59487",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples, max_length_inputs=1024, max_length_labels=128):\n",
    "    inputs = [\"summarize: \" + doc for doc in examples[\"text\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_length_inputs, truncation=True)\n",
    "    \n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"summary\"], max_length=max_length_labels, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3da66b4-525a-44c3-b977-79cd551cd746",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'summary', 'title'],\n",
       "        num_rows: 15159\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'summary', 'title'],\n",
       "        num_rows: 3790\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7758b6ab-af5e-491b-9588-a5af12067eb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'summary', 'title'],\n",
       "        num_rows: 1895\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'summary', 'title'],\n",
       "        num_rows: 1895\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2501cffa-7ce2-4098-a60e-436eda6eaa2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58d85924680a46d5bf4ccc66b7a71345",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a0135397f5d436cb14902acfbf6b014",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train = train_test_valid.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad75da53-d32e-4766-a13e-818d87fae48f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'summary', 'title', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 15159\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'summary', 'title', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3790\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8af14c3-31d6-44d1-af1c-88ca057cc7e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea8400cdfb6a44ef8671a351aa148ca2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_validation = test_valid[\"train\"].map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8487db81-7621-4cc8-b4b5-530a1ef23ec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'summary', 'title', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 1895\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "446db552-76b8-4a55-a765-7dddbb51962f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28ed6437f53c491c837d9a2986742f3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_test = test_valid[\"test\"].map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ae3e0ce-7806-440c-9eb5-885723424020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'summary', 'title', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 1895\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62701b8-f656-402c-83e8-87e95c8231b0",
   "metadata": {},
   "source": [
    "### Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "040b2486-72da-4b0d-ac6f-9a7b14b2abdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dec17cb-2e33-47a1-95a9-c3ae8539428b",
   "metadata": {},
   "source": [
    "### Evaluation Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5fb90b87-9980-4a53-a536-be639a55df85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c9a9077a7bc44eb9b76577cc327b3d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d67b19e-6d11-4509-b47a-83313ab3342b",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa31ec62-1545-4440-9ff7-c062c86c5ead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbcab7fa2c2b48c1b6548ec43a1a7ff9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4ac5cb201b147a79f0a68d42f7df538",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/231M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7ea0b9-1775-4059-90b0-b070403dcaa8",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ce8b29-8b16-45fc-9b90-c19d62446e28",
   "metadata": {},
   "source": [
    "### Hyper-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "91024ce7-02b6-4fdf-a677-51ebc3b91de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./output/results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=8,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "25b40d33-a5a1-45f7-baa3-4f4dad85b855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train[\"train\"],\n",
    "    eval_dataset=tokenized_validation,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5667c4d8-552b-478c-8df3-3cc10553b9f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: title, summary, text. If title, summary, text are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 15159\n",
      "  Num Epochs = 8\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 7584\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/notebooks/pretrained/wandb/run-20231122_150143-3mlw0qcc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/lambton-college/huggingface/runs/3mlw0qcc\" target=\"_blank\">./output/results</a></strong> to <a href=\"https://wandb.ai/lambton-college/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7584' max='7584' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7584/7584 2:59:22, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.362100</td>\n",
       "      <td>1.754534</td>\n",
       "      <td>0.225700</td>\n",
       "      <td>0.175700</td>\n",
       "      <td>0.215100</td>\n",
       "      <td>0.215100</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.909100</td>\n",
       "      <td>1.665250</td>\n",
       "      <td>0.232200</td>\n",
       "      <td>0.182400</td>\n",
       "      <td>0.222900</td>\n",
       "      <td>0.223100</td>\n",
       "      <td>18.999500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.836500</td>\n",
       "      <td>1.621972</td>\n",
       "      <td>0.235200</td>\n",
       "      <td>0.185500</td>\n",
       "      <td>0.226800</td>\n",
       "      <td>0.226800</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.782100</td>\n",
       "      <td>1.594288</td>\n",
       "      <td>0.236000</td>\n",
       "      <td>0.186600</td>\n",
       "      <td>0.227800</td>\n",
       "      <td>0.227900</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.753100</td>\n",
       "      <td>1.574268</td>\n",
       "      <td>0.236700</td>\n",
       "      <td>0.188000</td>\n",
       "      <td>0.228500</td>\n",
       "      <td>0.228500</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.733700</td>\n",
       "      <td>1.564112</td>\n",
       "      <td>0.236900</td>\n",
       "      <td>0.188700</td>\n",
       "      <td>0.228900</td>\n",
       "      <td>0.228900</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.722100</td>\n",
       "      <td>1.557661</td>\n",
       "      <td>0.237600</td>\n",
       "      <td>0.189500</td>\n",
       "      <td>0.229300</td>\n",
       "      <td>0.229400</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.718100</td>\n",
       "      <td>1.555388</td>\n",
       "      <td>0.237500</td>\n",
       "      <td>0.189500</td>\n",
       "      <td>0.229300</td>\n",
       "      <td>0.229400</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/results/checkpoint-500\n",
      "Configuration saved in ./output/results/checkpoint-500/config.json\n",
      "Model weights saved in ./output/results/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/results/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./output/results/checkpoint-500/special_tokens_map.json\n",
      "Copy vocab file to ./output/results/checkpoint-500/spiece.model\n",
      "Deleting older checkpoint [output/results/checkpoint-3500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: title, summary, text. If title, summary, text are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1895\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/results/checkpoint-1000\n",
      "Configuration saved in ./output/results/checkpoint-1000/config.json\n",
      "Model weights saved in ./output/results/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/results/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/results/checkpoint-1000/special_tokens_map.json\n",
      "Copy vocab file to ./output/results/checkpoint-1000/spiece.model\n",
      "Deleting older checkpoint [output/results/checkpoint-4000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/results/checkpoint-1500\n",
      "Configuration saved in ./output/results/checkpoint-1500/config.json\n",
      "Model weights saved in ./output/results/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/results/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in ./output/results/checkpoint-1500/special_tokens_map.json\n",
      "Copy vocab file to ./output/results/checkpoint-1500/spiece.model\n",
      "Deleting older checkpoint [output/results/checkpoint-4500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: title, summary, text. If title, summary, text are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1895\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/results/checkpoint-2000\n",
      "Configuration saved in ./output/results/checkpoint-2000/config.json\n",
      "Model weights saved in ./output/results/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/results/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/results/checkpoint-2000/special_tokens_map.json\n",
      "Copy vocab file to ./output/results/checkpoint-2000/spiece.model\n",
      "Deleting older checkpoint [output/results/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/results/checkpoint-2500\n",
      "Configuration saved in ./output/results/checkpoint-2500/config.json\n",
      "Model weights saved in ./output/results/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/results/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in ./output/results/checkpoint-2500/special_tokens_map.json\n",
      "Copy vocab file to ./output/results/checkpoint-2500/spiece.model\n",
      "Deleting older checkpoint [output/results/checkpoint-1000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: title, summary, text. If title, summary, text are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1895\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/results/checkpoint-3000\n",
      "Configuration saved in ./output/results/checkpoint-3000/config.json\n",
      "Model weights saved in ./output/results/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/results/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/results/checkpoint-3000/special_tokens_map.json\n",
      "Copy vocab file to ./output/results/checkpoint-3000/spiece.model\n",
      "Deleting older checkpoint [output/results/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/results/checkpoint-3500\n",
      "Configuration saved in ./output/results/checkpoint-3500/config.json\n",
      "Model weights saved in ./output/results/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/results/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in ./output/results/checkpoint-3500/special_tokens_map.json\n",
      "Copy vocab file to ./output/results/checkpoint-3500/spiece.model\n",
      "Deleting older checkpoint [output/results/checkpoint-2000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: title, summary, text. If title, summary, text are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1895\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/results/checkpoint-4000\n",
      "Configuration saved in ./output/results/checkpoint-4000/config.json\n",
      "Model weights saved in ./output/results/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/results/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/results/checkpoint-4000/special_tokens_map.json\n",
      "Copy vocab file to ./output/results/checkpoint-4000/spiece.model\n",
      "Deleting older checkpoint [output/results/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/results/checkpoint-4500\n",
      "Configuration saved in ./output/results/checkpoint-4500/config.json\n",
      "Model weights saved in ./output/results/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/results/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in ./output/results/checkpoint-4500/special_tokens_map.json\n",
      "Copy vocab file to ./output/results/checkpoint-4500/spiece.model\n",
      "Deleting older checkpoint [output/results/checkpoint-3000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: title, summary, text. If title, summary, text are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1895\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/results/checkpoint-5000\n",
      "Configuration saved in ./output/results/checkpoint-5000/config.json\n",
      "Model weights saved in ./output/results/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/results/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/results/checkpoint-5000/special_tokens_map.json\n",
      "Copy vocab file to ./output/results/checkpoint-5000/spiece.model\n",
      "Deleting older checkpoint [output/results/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/results/checkpoint-5500\n",
      "Configuration saved in ./output/results/checkpoint-5500/config.json\n",
      "Model weights saved in ./output/results/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/results/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in ./output/results/checkpoint-5500/special_tokens_map.json\n",
      "Copy vocab file to ./output/results/checkpoint-5500/spiece.model\n",
      "Deleting older checkpoint [output/results/checkpoint-4000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: title, summary, text. If title, summary, text are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1895\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/results/checkpoint-6000\n",
      "Configuration saved in ./output/results/checkpoint-6000/config.json\n",
      "Model weights saved in ./output/results/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/results/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/results/checkpoint-6000/special_tokens_map.json\n",
      "Copy vocab file to ./output/results/checkpoint-6000/spiece.model\n",
      "Deleting older checkpoint [output/results/checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/results/checkpoint-6500\n",
      "Configuration saved in ./output/results/checkpoint-6500/config.json\n",
      "Model weights saved in ./output/results/checkpoint-6500/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/results/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in ./output/results/checkpoint-6500/special_tokens_map.json\n",
      "Copy vocab file to ./output/results/checkpoint-6500/spiece.model\n",
      "Deleting older checkpoint [output/results/checkpoint-5000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: title, summary, text. If title, summary, text are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1895\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./output/results/checkpoint-7000\n",
      "Configuration saved in ./output/results/checkpoint-7000/config.json\n",
      "Model weights saved in ./output/results/checkpoint-7000/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/results/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/results/checkpoint-7000/special_tokens_map.json\n",
      "Copy vocab file to ./output/results/checkpoint-7000/spiece.model\n",
      "Deleting older checkpoint [output/results/checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./output/results/checkpoint-7500\n",
      "Configuration saved in ./output/results/checkpoint-7500/config.json\n",
      "Model weights saved in ./output/results/checkpoint-7500/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/results/checkpoint-7500/tokenizer_config.json\n",
      "Special tokens file saved in ./output/results/checkpoint-7500/special_tokens_map.json\n",
      "Copy vocab file to ./output/results/checkpoint-7500/spiece.model\n",
      "Deleting older checkpoint [output/results/checkpoint-6000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: title, summary, text. If title, summary, text are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1895\n",
      "  Batch size = 16\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7584, training_loss=1.829566307711702, metrics={'train_runtime': 10786.3099, 'train_samples_per_second': 11.243, 'train_steps_per_second': 0.703, 'total_flos': 3.282634189622477e+16, 'train_loss': 1.829566307711702, 'epoch': 8.0})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3385d6-f428-4f92-bace-4374cf60a1fd",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0f130dd9-1463-4bf2-bcc3-1f6eaff631f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: title, summary, text. If title, summary, text are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1895\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='119' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/119 01:59]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.5680652856826782,\n",
       " 'eval_rouge1': 0.2362,\n",
       " 'eval_rouge2': 0.1893,\n",
       " 'eval_rougeL': 0.2277,\n",
       " 'eval_rougeLsum': 0.2277,\n",
       " 'eval_gen_len': 19.0,\n",
       " 'eval_runtime': 128.472,\n",
       " 'eval_samples_per_second': 14.75,\n",
       " 'eval_steps_per_second': 0.926,\n",
       " 'epoch': 8.0}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(eval_dataset=tokenized_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98166a0-bb9f-45f2-9abb-f52c2e5614fd",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "45084fd9-de67-4da8-82a9-2deaedd3da9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"summarize: The Inflation Reduction Act lowers prescription drug costs, \n",
    "health care costs, and energy costs. It's the most aggressive action on tackling the \n",
    "climate crisis in American history, which will lift up American workers and create good-paying, \n",
    "union jobs across the country. It'll lower the deficit and ask the ultra-wealthy and \n",
    "corporations to pay their fair share. And no one making under $400,000 per year will \n",
    "pay a penny more in taxes.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae0c226-7ed1-46c0-9019-5809d5b21017",
   "metadata": {},
   "source": [
    "### Load Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "488a02f9-db19-4345-80b5-2c81db8a9feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Didn't find file output/results/checkpoint-7500/added_tokens.json. We won't load it.\n",
      "loading file output/results/checkpoint-7500/spiece.model\n",
      "loading file output/results/checkpoint-7500/tokenizer.json\n",
      "loading file None\n",
      "loading file output/results/checkpoint-7500/special_tokens_map.json\n",
      "loading file output/results/checkpoint-7500/tokenizer_config.json\n",
      "loading configuration file output/results/checkpoint-7500/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"output/results/checkpoint-7500\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file output/results/checkpoint-7500/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at output/results/checkpoint-7500.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model_output = \"output/results/checkpoint-7500\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_output)\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "model_ = AutoModelForSeq2SeqLM.from_pretrained(model_output)\n",
    "outputs = model_.generate(inputs, max_new_tokens=100, do_sample=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8117e56-e0ce-4a1e-a613-b0694fd4d0d2",
   "metadata": {},
   "source": [
    "### Decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9321fd9b-a3d6-4ab7-bb91-302d141c20a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Inflation Reduction Act aims to lower prescription drug costs, health care costs, and energy costs.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "df37adca-13ac-46a7-beb9-d7a67f8c3d3d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'flour confection made from flour, sugar, and other ingredients and is usually baked.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"summarize: Cake is a flour confection made from flour, sugar, and other ingredients and is usually baked. \n",
    "In their oldest forms, cakes were modifications of bread, but cakes now cover a wide range of preparations \n",
    "that can be simple or elaborate and which share features with desserts such as pastries, meringues, custards, and pies.\n",
    "The most common ingredients include flour, sugar, eggs, fat (such as butter, oil, or margarine), a liquid, \n",
    "and a leavening agent, such as baking soda or baking powder. Common additional ingredients include dried, candied, or \n",
    "fresh fruit, nuts, cocoa, and extracts such as vanilla, with numerous substitutions for the primary ingredients. Cakes \n",
    "can also be filled with fruit preserves, nuts, or dessert sauces (like custard, jelly, cooked fruit, whipped cream, or \n",
    "syrups), iced with buttercream or other icings, and decorated with marzipan, piped borders, or candied fruit.\"\"\"\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "outputs = model_.generate(inputs, max_new_tokens=100, do_sample=False)\n",
    "tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0ebcbbff-7104-4a06-90c3-5aa27fb4c075",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"31 minutos is a Chilean comedy television series and a children's music virtual band created by the production company Aplaplac (owned by lvaro Daz, Pedro Peirano and Juan Manuel Egaa) that began to be transmitted on March 15, 2003 by the signal of Televisión Nacional de Chile (TVN). The program is a parody to 60 minutos, a controversial news program\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"summarize: 31 minutos (English: 31 minutes) is a Chilean comedy television series and a children's music \n",
    "virtual band created by the production company Aplaplac (owned by Álvaro Díaz, Pedro Peirano and Juan Manuel Egaña) \n",
    "that began to be transmitted on March 15, 2003 by the signal of Televisión Nacional de Chile (TVN). The program \n",
    "is a parody to 60 minutos, a controversial news program of the same channel, transmitted in the 1970s and 1980s. It \n",
    "focuses on the adventures of the team of a news program of little prestige in which something unexpected \n",
    "always happens, whose presenter is Tulio Triviño. The program's notes are educational and leave an explicit \n",
    "or implicit message, while others are quite ridiculous.\n",
    "\n",
    "In its first period, the series had three seasons, from 2003 to 2005, in addition to a participation for the 2003 Chilean \n",
    "Telethon and a Christmas special that same year. On March 27, 2008, the series was taken to the cinema under the \n",
    "title of 31 minutos, la película.\n",
    "\n",
    "After the third season and for the next nine years the series had no new episodes. In 2012, the production company \n",
    "Aplaplac confirmed that the series would return to television with a fourth season, which was released on \n",
    "October 4, 2014 through TVN, and its last original episode was broadcast on the night of December 27, 2014. \n",
    "During its run, the series received universal acclaim from critics and viewers alike, with praise directed to its \n",
    "clever humour, soundtrack, accessibility towards children about complex issues and helping to revitalize the \n",
    "Chilean puppetry tradition.\n",
    "\n",
    "From 2004 to 2007, it was broadcast throughout Latin America by Nickelodeon and from 2015, it began to be broadcast \n",
    "by Cartoon Network. It also broadcasts in Mexico on Canal Once and Once Niños, and its most recent season is \n",
    "available in the Netflix Latin America catalog.\n",
    "\n",
    "31 minutos has performed throughout Chile and Mexico, making the program a musical band. On their tours they \n",
    "perform the songs broadcast on the program and their musical works outside of it.\"\"\"\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "outputs = model_.generate(inputs, max_new_tokens=100, do_sample=False)\n",
    "tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a6fac915-23e0-4dfc-b3fc-4ef6d099f9fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "output/results/checkpoint-7500/\n",
      "output/results/checkpoint-7500/trainer_state.json\n",
      "output/results/checkpoint-7500/pytorch_model.bin\n",
      "output/results/checkpoint-7500/training_args.bin\n",
      "output/results/checkpoint-7500/optimizer.pt\n",
      "output/results/checkpoint-7500/tokenizer.json\n",
      "output/results/checkpoint-7500/config.json\n",
      "output/results/checkpoint-7500/rng_state.pth\n",
      "output/results/checkpoint-7500/scheduler.pt\n",
      "output/results/checkpoint-7500/tokenizer_config.json\n",
      "output/results/checkpoint-7500/scaler.pt\n",
      "output/results/checkpoint-7500/special_tokens_map.json\n",
      "output/results/checkpoint-7500/spiece.model\n"
     ]
    }
   ],
   "source": [
    "!tar -zcvf \"pretrained.tar.gz\" \"output/results/checkpoint-7500\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9ee8e8-edb7-4f5f-a2be-2e5b18fb550e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
